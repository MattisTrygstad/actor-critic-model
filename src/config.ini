[parameters]

# --- PROGRAM FLOW --- #
human_mode = False
experiments = True

# --- EXPERIMENT PARAMETERS --- #
actor_learning_rates = [0.2, 0.7, 0.8]
critic_learning_rates = [0.1, 0.2]
decay_discount_values = [0.8, 0.9]
win_multipliers = [100]
initial_epsilons = [0.8]
exploitation_thresholds = [1]
iterations = 50


# --- ENVIRONMENT CONFIGURATION --- #

# -- TASK 2: 4x4 diamond -- #
action_reward = 0
win_multiplier = 100
reinforcement = 1
board_type = triangle
board_size = 5
empty_nodes = [(2, 1)]

# -- TASK 3: 4x4 diamond -- #
; action_reward = 0
; win_reward = 1
; loss_reward = 0
; # 0: triangle, 1: diamond
; board_type = 1
; board_size = 4
; empty_nodes = [(3,1)]
; win_multiplier = 100


# --- LEARNING CONFIGURATION --- #
nn_critic = False
episodes = 600
test_episodes = 10
nn_dimentions = [50]
actor_learning_rate = 0.7
critic_learning_rate = 0.1
actor_decay_rate = 0.9
critic_decay_rate = 0.8
actor_discount_factor = 0.95
critic_discount_factor = 0.8


# --- EPSILON CONFIGURATION --- #
linear_epsilon = False
exploitation_threshold = 1
epsilon = 0.8
epsilon_decay = 0.985


# --- VISUALIZATION CONFIGURATION --- #
visualize_without_convergence = True
visualization_delay = 0.5
